{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install google-generativeai python-dotenv openai gradio langchain ",
   "id": "9df3ae85625de219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from click import prompt\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import textwrap\n",
    "import PIL.Image\n",
    "import google.generativeai as genai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import base64\n",
    "from google.colab import userdata"
   ],
   "id": "8e41e158bc88d5fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### *Load gemmini (LLM) client*",
   "id": "ba12227fe37e95f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "_ = load_dotenv(find_dotenv('../config/.env')) # read local .env file\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') if GOOGLE_API_KEY == None else GOOGLE_API_KEY\n",
    "\n",
    "print(f\"The Gemmi API KEY found is {GOOGLE_API_KEY}\")\n",
    "#Configure API KEY GEMMINI\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ],
   "id": "8c796e4c147c1d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SYSTEM_PROMPT = \"\"\n",
    "\n",
    "SYSTEM_PROMPT_IDENTIFIED = \"Explain what you see in this image.\""
   ],
   "id": "33f6fca644d089a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transform image or video to base64",
   "id": "34bbbdd806e4b445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Helper function to convert a file to base64 representation\n",
    "def toBase64(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')"
   ],
   "id": "8176c06f644bfe90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### *Functions for call LLM request*",
   "id": "57f8caad80d37366"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def to_markdown(text):\n",
    "    text = text.replace('â€¢', '  *')\n",
    "    return textwrap.indent(text, '> ', predicate=lambda _: True)\n",
    "\n",
    "\n",
    "def call_Gemmi_LLM(image_path: str, prompt: str) -> str:\n",
    "    # Load the image\n",
    "    # img = PIL.Image.open(image_path)\n",
    "\n",
    "    sample_file = genai.upload_file(path=image_path, display_name=\"Sample drawing\")\n",
    "\n",
    "    #\n",
    "    # Call generative model\n",
    "    #\n",
    "    #\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "        [prompt, sample_file]\n",
    "    )\n",
    "\n",
    "    return to_markdown(response.text)\n",
    "\n",
    "def inference(prompt, inp_img, inp_video, token):\n",
    "    time.sleep(3)\n",
    "    result_call_video_gemmini = result_call_image_gemmini = ''\n",
    "    prompt = SYSTEM_PROMPT_IDENTIFIED\n",
    "\n",
    "    if inp_img != '' and inp_img is not None:\n",
    "      result_call_image_gemmini = call_Gemmi_LLM(inp_img, prompt)\n",
    "\n",
    "    if inp_video != '' and inp_video is not None:\n",
    "        print(f\"Video Address to show and index ${inp_video}\")\n",
    "        result_call_video_gemmini = call_Gemmi_LLM(inp_video, prompt)\n",
    "\n",
    "    return [f\" Prompt {prompt} \\n\\n Image:\\n {result_call_image_gemmini},\\n\\n\\n Video:\\n {result_call_video_gemmini}\",\"B\"]\n",
    "\n",
    "def multimodalraginference(*args):\n",
    "    gr.Warning(\"Building action!!!!\")\n",
    "    return\n",
    "\n",
    "def multimodalrecomendation(*args):\n",
    "    gr.Warning(\"Building action!!!!\")\n",
    "    return"
   ],
   "id": "8847e96a320048bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weviate client request",
   "id": "3078aed12ee0ca32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create gradio UI",
   "id": "1f429951bc6ffab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#########################################\n",
    "#\n",
    "#   Bibliografy:\n",
    "#     Google AI Generative AI with Weaviate (https://weaviate.io/developers/weaviate/model-providers/google/generative)\n",
    "#\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "\n",
    "with gr.Blocks(theme = 'JohnSmith9982/small_and_pretty') as demo:\n",
    "    with gr.Tab(\"Multimodal Search \"):\n",
    "        message = (\"### Use LLM Multimodal Prompt search with Gradio using these \\\n",
    "        [video tutorial](https://www.youtube.com/watch?v=ABNxNFPqIGQ&t=4s) and Hugging Face API\")\n",
    "        gr.Markdown(\"<center><h2>Open LLM Explorer</h2></center>\")\n",
    "        gr.Markdown(message)\n",
    "\n",
    "        prompt = gr.Textbox(label = 'Prompt', lines= 3, max_lines = 5, value = SYSTEM_PROMPT_IDENTIFIED)\n",
    "        with gr.Row():\n",
    "            # Image gradio component bibliography https://www.gradio.app/docs/gradio/image\n",
    "            #\n",
    "            inp_img = gr.Image(label=\"Image to Search\",type=\"filepath\")\n",
    "\n",
    "            # Video gradio component bibliography https://www.gradio.app/docs/gradio/video\n",
    "            #\n",
    "            #\n",
    "            inp_video = gr.Video(label=\"Video to Search\")\n",
    "        token = gr.Textbox(label='Token', type='password')\n",
    "\n",
    "\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                generate_btn = gr.Button(\"Generate\", size = \"lg\", variant = \"primary\")\n",
    "                code_btn = gr.Button(\"View Code\", size = \"lg\", variant = \"secondary\")\n",
    "\n",
    "        with gr.Row() as row_output:\n",
    "           llama_output = gr.Markdown(\"##Llama3.1 70B-instruct Output\")\n",
    "           groq_output = gr.Markdown(\"##Groq with Llama3.1 70B-instruct Output\")\n",
    "\n",
    "        generate_btn.click(fn= inference, inputs=[prompt, inp_img, inp_video, token],outputs=[llama_output,groq_output])\n",
    "    with gr.Tab(\"RAG Multimodal\"):\n",
    "        ragMultimodal_button = gr.Button(\"In Building\")\n",
    "\n",
    "        ragMultimodal_button.click(fn = multimodalraginference)\n",
    "    with gr.Tab(\"Multimodal Recommendation\"):\n",
    "        ragMultimodal_button = gr.Button(\"In Building\")\n",
    "\n",
    "        ragMultimodal_button.click(fn = multimodalrecomendation)\n",
    "\n",
    "\n",
    "demo.launch()\n"
   ],
   "id": "d2788d7cbe61c01d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
